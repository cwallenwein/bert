{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-07-30T08:36:32.908375Z",
     "start_time": "2024-07-30T08:36:22.748716Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/bert/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training loss:  11.0406:   4%|‚ñç         | 43/1000 [00:04<01:28, 10.76 steps/s]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[1], line 18\u001B[0m\n\u001B[1;32m     16\u001B[0m training_args \u001B[38;5;241m=\u001B[39m TrainingArguments(batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m, learning_rate\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1e-4\u001B[39m)\n\u001B[1;32m     17\u001B[0m trainer \u001B[38;5;241m=\u001B[39m TrainerForPreTraining(model, training_args, verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[0;32m---> 18\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain(training_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1000\u001B[39m, context_length\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m128\u001B[39m, dataset_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m800M_tokens\u001B[39m\u001B[38;5;124m\"\u001B[39m, experiment_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtest_run\u001B[39m\u001B[38;5;124m\"\u001B[39m, with_wandb\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n",
      "File \u001B[0;32m~/Code/bert/trainer/trainer.py:71\u001B[0m, in \u001B[0;36mTrainerForPreTraining.train\u001B[0;34m(self, training_steps, dataset_name, experiment_name, context_length, with_nsp, with_wandb)\u001B[0m\n\u001B[1;32m     69\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m step \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(training_steps):\n\u001B[1;32m     70\u001B[0m     batch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mnext\u001B[39m(dataset)\n\u001B[0;32m---> 71\u001B[0m     total_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstep(\n\u001B[1;32m     72\u001B[0m         batch,\n\u001B[1;32m     73\u001B[0m         with_nsp\u001B[38;5;241m=\u001B[39mwith_nsp,\n\u001B[1;32m     74\u001B[0m         with_wandb\u001B[38;5;241m=\u001B[39mwith_wandb\n\u001B[1;32m     75\u001B[0m     )\n\u001B[1;32m     76\u001B[0m     progress_bar\u001B[38;5;241m.\u001B[39mset_description(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTraining loss: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtotal_loss\u001B[38;5;132;01m:\u001B[39;00m\u001B[38;5;124m .4f\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     77\u001B[0m     progress_bar\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;241m1\u001B[39m)\n",
      "File \u001B[0;32m~/Code/bert/trainer/trainer.py:94\u001B[0m, in \u001B[0;36mTrainerForPreTraining.step\u001B[0;34m(self, batch, with_nsp, with_wandb)\u001B[0m\n\u001B[1;32m     91\u001B[0m batch_metrics \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m()\n\u001B[1;32m     93\u001B[0m batch_mlm_loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcalculate_mlm_loss(batch, masked_language_modeling_output)\n\u001B[0;32m---> 94\u001B[0m batch_mlm_acc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcalculate_mlm_acc(batch, masked_language_modeling_output)\n\u001B[1;32m     96\u001B[0m batch_total_loss \u001B[38;5;241m=\u001B[39m batch_mlm_loss\n\u001B[1;32m     98\u001B[0m batch_metrics[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmlm_loss\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m batch_mlm_loss\u001B[38;5;241m.\u001B[39mitem()\n",
      "File \u001B[0;32m~/Code/bert/trainer/trainer.py:161\u001B[0m, in \u001B[0;36mTrainerForPreTraining.calculate_mlm_acc\u001B[0;34m(self, batch, masked_language_modeling_output)\u001B[0m\n\u001B[1;32m    158\u001B[0m masked_token_labels \u001B[38;5;241m=\u001B[39m batch[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m\"\u001B[39m][masked_tokens]\n\u001B[1;32m    160\u001B[0m \u001B[38;5;66;03m# calculate accuracy\u001B[39;00m\n\u001B[0;32m--> 161\u001B[0m mlm_acc \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmasked_language_modeling_accuracy(\n\u001B[1;32m    162\u001B[0m     masked_token_predictions, masked_token_labels\n\u001B[1;32m    163\u001B[0m )\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m mlm_acc\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/bert/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1530\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m   1531\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1532\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/bert/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1536\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1537\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[1;32m   1539\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1540\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1541\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1543\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1544\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/bert/lib/python3.12/site-packages/torchmetrics/metric.py:311\u001B[0m, in \u001B[0;36mMetric.forward\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    309\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_cache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_full_state_update(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    310\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 311\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_cache \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_reduce_state_update(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    313\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_cache\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/bert/lib/python3.12/site-packages/torchmetrics/metric.py:380\u001B[0m, in \u001B[0;36mMetric._forward_reduce_state_update\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    377\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_enable_grad \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m  \u001B[38;5;66;03m# allow grads for batch computation\u001B[39;00m\n\u001B[1;32m    379\u001B[0m \u001B[38;5;66;03m# calculate batch state and compute batch value\u001B[39;00m\n\u001B[0;32m--> 380\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mupdate(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    381\u001B[0m batch_val \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute()\n\u001B[1;32m    383\u001B[0m \u001B[38;5;66;03m# reduce batch and global state\u001B[39;00m\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/bert/lib/python3.12/site-packages/torchmetrics/metric.py:482\u001B[0m, in \u001B[0;36mMetric._wrap_update.<locals>.wrapped_func\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    480\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mset_grad_enabled(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_enable_grad):\n\u001B[1;32m    481\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 482\u001B[0m         update(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    483\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m err:\n\u001B[1;32m    484\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected all tensors to be on\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(err):\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/bert/lib/python3.12/site-packages/torchmetrics/classification/stat_scores.py:343\u001B[0m, in \u001B[0;36mMulticlassStatScores.update\u001B[0;34m(self, preds, target)\u001B[0m\n\u001B[1;32m    339\u001B[0m     _multiclass_stat_scores_tensor_validation(\n\u001B[1;32m    340\u001B[0m         preds, target, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_classes, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmultidim_average, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mignore_index\n\u001B[1;32m    341\u001B[0m     )\n\u001B[1;32m    342\u001B[0m preds, target \u001B[38;5;241m=\u001B[39m _multiclass_stat_scores_format(preds, target, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtop_k)\n\u001B[0;32m--> 343\u001B[0m tp, fp, tn, fn \u001B[38;5;241m=\u001B[39m _multiclass_stat_scores_update(\n\u001B[1;32m    344\u001B[0m     preds, target, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_classes, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtop_k, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maverage, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmultidim_average, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mignore_index\n\u001B[1;32m    345\u001B[0m )\n\u001B[1;32m    346\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_state(tp, fp, tn, fn)\n",
      "File \u001B[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/bert/lib/python3.12/site-packages/torchmetrics/functional/classification/stat_scores.py:401\u001B[0m, in \u001B[0;36m_multiclass_stat_scores_update\u001B[0;34m(preds, target, num_classes, top_k, average, multidim_average, ignore_index)\u001B[0m\n\u001B[1;32m    399\u001B[0m     preds \u001B[38;5;241m=\u001B[39m preds[idx]\n\u001B[1;32m    400\u001B[0m     target \u001B[38;5;241m=\u001B[39m target[idx]\n\u001B[0;32m--> 401\u001B[0m tp \u001B[38;5;241m=\u001B[39m (preds \u001B[38;5;241m==\u001B[39m target)\u001B[38;5;241m.\u001B[39msum()\n\u001B[1;32m    402\u001B[0m fp \u001B[38;5;241m=\u001B[39m (preds \u001B[38;5;241m!=\u001B[39m target)\u001B[38;5;241m.\u001B[39msum()\n\u001B[1;32m    403\u001B[0m fn \u001B[38;5;241m=\u001B[39m (preds \u001B[38;5;241m!=\u001B[39m target)\u001B[38;5;241m.\u001B[39msum()\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from trainer import TrainerForPreTraining\n",
    "from trainer.arguments import TrainingArguments\n",
    "from model import BertModelForPretraining\n",
    "from model.config import BertConfig\n",
    "from data import BertDataset\n",
    "\n",
    "dataset = BertDataset.load(preprocessed_name=\"800M_tokens\", context_length=128, verbose=False)\n",
    "\n",
    "config = BertConfig(\n",
    "    d_model=32,\n",
    "    n_layers=2,\n",
    "    context_length=512,\n",
    "    n_heads=4,\n",
    "    multi_head_attention_implementation=\"pytorch\",\n",
    ")\n",
    "\n",
    "model = BertModelForPretraining(config)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    micro_batch_size=8,\n",
    "    macro_batch_size=32,\n",
    "    learning_rate=1e-4,\n",
    "    device=\"mps\",\n",
    "    with_wandb=False\n",
    ")\n",
    "trainer = TrainerForPreTraining(\"test_run\", training_args, verbose=False)\n",
    "trainer.train(training_steps=1000, dataset=dataset, model=model, with_nsp=True)\n",
    "# trainer.train(training_steps=10, context_length=512, dataset_name=\"800M_tokens\", experiment_name=\"test_run\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
